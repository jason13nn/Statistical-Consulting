---
title: "HW6_6366"
author: "Yichien Chou"
date: "2020/10/21"
output: html_document
---

```{r, echo=FALSE}
#read data
cars <- read.csv("/Users/jason13nn/Desktop/SMU/Fall 2020/ST 6366/HW/HW6_Imputation/cars.csv")

#subset data
cars <- cars[,2:6]

str(cars)
```

#Queston 1: MLR regression model

```{r, message=FALSE, echo=FALSE}
cars.lm <- lm(MPG ~., cars)
summary(cars.lm)
```

There were only 22 rows were used for this analysis since 16 observations were deleted due to missingness.

#Question 2: md.pattern(), md.pairs(), and marginplot()

##md.pattern()

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(mice)
library(VIM)

#Number of missing values
md.pattern(cars)
```

The plot above indicates that 22 out of 38 rows are complete, 4 missing **weight**, 5 missing **HP**, 3 missing **cylinders**, 1 missing **cylinders** and **weight**, 2 missing **size**, 1 missing **size** and **weight**.

Total missing: (1 x 2) + (2 x 1) + (1 x 2) + (3 x 1) + (5 x 1) + (4 x 1) = 18.

##md.pairs()

```{r, echo=FALSE}
#Number of observations per pair
md.pairs(cars)
```

The output shows the observations by four groups: 

* $rr: Both variables observed. 

* $rm: First observed, second missing. 

* $mr: First missing, second observed.

* $mm: Both variables missing. 

##marginplot()

```{r, echo=FALSE}
#Marginal plot of size and weight
marginplot(cars[, c("SIZE", "WEIGHT")], col = mdc(1:2), cex = 1.2, cex.lab = 1.2, cex.numbers = 1.3, pch = 19)
```

* 2 points in left margin indicate **weight** observed but **size** missing. 

* 5 points in botton margin indicate **size** observed but **weight** missing.

* 1 point interected indicates both **size** and **weight** missing.

#Question 3: Imputation

##Create Imputations

I applied *Bayesian linear regression*, *predictive mean matching*, and *linear regression using bootstrap* as imputation methods. 

My primary goal is to compare *pmm* and *norm.boot* using *mice* package. Let **combination 1** be the combinations of imputation *Bayesian linear regression* and *predictive mean matching*, **combination 2** be the combinations of imputation *Bayesian linear regression* and *linear regression using bootstrap*.

```{r, message=FALSE, results='hide', echo=FALSE}
#1. pmm
cars.impute.1 <- mice(cars, m =20, method = c("norm",  "pmm", "pmm", "pmm", "pmm"), seed = 520, maxit = 30)

#2. norm.boot
cars.impute.2 <- mice(cars, m =20, method = c("norm",  "norm.boot", "norm.boot", "norm.boot", "norm.boot"), seed = 520, maxit = 30)
```

```{r,, echo=FALSE}
##Information about imputations
print(cars.impute.1)
print(cars.impute.2)
```

##Diagnostic Checks

###Distributions of variables as individual points

####	Predictive mean matching

```{r, echo=FALSE}
stripplot(cars.impute.1, pch = 20, cex = 1.2)
```

####Linear regression using bootstrap

```{r, echo=FALSE}
stripplot(cars.impute.2, pch = 20, cex = 1.2)
```

Blue points are observed, the red points are imputed. The red points follow the blue points reasonably well, including the gaps in the distribution. 

###Bivariate plots of raw and all imputed data sets

####Predictive mean matching

```{r, echo=FALSE}
xyplot(cars.impute.1, MPG ~  CYLINDERS| .imp, pch = 20, cex = 1.4)
xyplot(cars.impute.1, MPG ~  SIZE| .imp, pch = 20, cex = 1.4)
xyplot(cars.impute.1, MPG ~  HP| .imp, pch = 20, cex = 1.4)
xyplot(cars.impute.1, MPG ~  WEIGHT| .imp, pch = 20, cex = 1.4)
```

####Linear regression using bootstrap

```{r, echo=FALSE}
xyplot(cars.impute.2, MPG ~  CYLINDERS| .imp, pch = 20, cex = 1.4)
xyplot(cars.impute.2, MPG ~  SIZE| .imp, pch = 20, cex = 1.4)
xyplot(cars.impute.2, MPG ~  HP| .imp, pch = 20, cex = 1.4)
xyplot(cars.impute.2, MPG ~  WEIGHT| .imp, pch = 20, cex = 1.4)
```

There is no significant difference between these two combinations.

###Check convergence

####m=5

```{r, echo=FALSE, results='hide'}
cars.impute.1 <- mice(cars, m =5, method = c("norm",  "pmm", "pmm", "pmm", "pmm"), seed = 520, maxit = 30)

cars.impute.2 <- mice(cars, m =5, method = c("norm",  "norm.boot", "norm.boot", "norm.boot", "norm.boot"), seed = 520, maxit = 30)
```
```{r, echo=FALSE}
plot(cars.impute.1)
plot(cars.impute.2)
```

####m=10

```{r, echo=FALSE, results='hide'}
cars.impute.1 <- mice(cars, m =10, method = c("norm",  "pmm", "pmm", "pmm", "pmm"), seed = 520, maxit = 30)

cars.impute.2 <- mice(cars, m =10, method = c("norm",  "norm.boot", "norm.boot", "norm.boot", "norm.boot"), seed = 520, maxit = 30)
```
```{r, echo=FALSE}
plot(cars.impute.1)
plot(cars.impute.2)
```

####m=20

```{r,echo=FALSE, results='hide'}
cars.impute.1 <- mice(cars, m =20, method = c("norm",  "pmm", "pmm", "pmm", "pmm"), seed = 520, maxit = 30)

cars.impute.2 <- mice(cars, m =20, method = c("norm",  "norm.boot", "norm.boot", "norm.boot", "norm.boot"), seed = 520, maxit = 30)
```
```{r, echo=FALSE}
plot(cars.impute.1)
plot(cars.impute.2)
```

Convergence is healthy when m=5, 10, and 20. However, **CYLINDER** seems non-convergence.

#Question 4: Pooling functionality

##Combibation 1

The regression coefficients and standard errors are as below.

```{r, echo=FALSE}
##Fit a linear regression model to each imputed data set
cars.imp.lm1 <- with(cars.impute.1, lm(MPG ~ CYLINDERS + SIZE + HP + WEIGHT))

##Pool the regression estimates using Rubin's rules
summary(pool(cars.imp.lm1))
```

###Combibation 2

The regression coefficients and standard errors are as below.

```{r, echo=FALSE}
cars.imp.lm2 <- with(cars.impute.2, lm(MPG ~ CYLINDERS + SIZE + HP + WEIGHT))

summary(pool(cars.imp.lm2))
```

#Question 5: Imputation vs. Listwise Deletion

To visualize the estimates of linear regression models I created, I prefer to use coefficient plots.

##Coefficient plot

###Listwise Deletion

```{r, echo=FALSE}
library(ggplot2)

###Coef Plot

#Listwise Deletion
coef.lm <- data.frame(coef(cars.lm))
coef_frame1 <- data.frame(coef = rownames(coef.lm)[-1], value = coef.lm[-1,1])

ggplot(coef_frame1, aes(x=coef, y=value)) +
geom_pointrange(aes(ymin=0, ymax=value), col = "black", fill = "blue") + 
  ggtitle("Coefficients of linear regression using Listwise Deletion") +
coord_flip() +
theme_bw()
```

**WEIGHT** had the largest coefficient.

###Combination 1

```{r, echo=FALSE}
#Combination1
coef.pmm <- pool(cars.imp.lm1)$pooled
coef_frame2 <- data.frame(coef = coef.pmm$term[-1], value = coef.pmm$estimate[-1])

ggplot(coef_frame2, aes(x=coef, y=value)) +
geom_pointrange(aes(ymin=0, ymax=value), col = "black", fill = "blue") + ggtitle("Coefficients of linear regression using Combination 1") +
coord_flip() +
theme_bw()
```

###Combination 2

```{r, echo=FALSE}
#Combination2
coef.norm.boot <- pool(cars.imp.lm2)$pooled
coef_frame3 <- data.frame(coef = coef.norm.boot$term[-1], value = coef.norm.boot$estimate[-1])

ggplot(coef_frame3, aes(x=coef, y=value)) +
geom_pointrange(aes(ymin=0, ymax=value), col = "black", fill = "blue") + ggtitle("Coefficients of linear regression using Combination 2") +
coord_flip() +
theme_bw()
```

From the coefficient plots above, we cannot find any significant difference between the three methods.

##Coefficient Table

```{r, echo=FALSE}
d <- data.frame(Method = c("Listwise Deletion", "Combination 1", "Combination 2"),
                CYLINDERS = c(coef_frame1[1,2], coef_frame2[1,2], coef_frame3[1,2]),
                SIZE = c(coef_frame1[2,2], coef_frame2[2,2], coef_frame3[2,2]),
                HP = c(coef_frame1[3,2], coef_frame2[3,2], coef_frame3[3,2]),
                WEIGHT = c(coef_frame1[4,2], coef_frame2[4,2], coef_frame3[4,2]))

knitr::kable(d)
```

The result shows similar estimates of **Listwise deletion** and **Combination 1**,  **Combination 2** has a higher coefficient in *SIZE* and a lower coefficient in *WEIGHT* compared to the former two methods.

#Question 6: Comparison of imputations

To compare these two combinations of imputations, I created a histogram of both combinations of imputations.

```{r, echo=FALSE}
library(lattice)

mpg<- c(complete(cars.impute.1)$MPG, complete(cars.impute.2)$MPG)

method <- rep(c("pmm", "norm.boot"), each = nrow(cars))

mpgm <- data.frame(mpg = mpg, method = method)

histogram(~mpg | method, data = mpgm, nint = 25)
```

There is no difference in the distribution between the two different imputation combinations.

#Question 7: Effects of variables

From the result of *Question 4*, we can find the intercept and slope for each variable in *cars*.

###Combination 1 (Bayesian linear regression and Predictive mean matching)

**SIZE** and **WEIGHT** are statistically significant (p<0.05). 

A 95 % confidence interval of **SIZE** is [0.0224, 0.1060].

A 95 % confidence interval of **WEIGHT** is [-17.4961, -7.1535].

We can find the same result from **combination 2**.

###Combination 2 (Bayesian linear regression and Linear regression using bootstrap)

**SIZE** and **WEIGHT** are statistically significant (p<0.05). 

A 95 % confidence interval of **SIZE** is [0.0334, 0.1159].

A 95 % confidence interval of **WEIGHT** is [-17.1506, -7.49901].

